{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "turned-asian",
   "metadata": {},
   "source": [
    "# Chinese NER Test Pipeline \n",
    "## By: Cameron B.\n",
    "\n",
    "### Download model from ModelScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dominant-nancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 14:22:23,904 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n"
     ]
    }
   ],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "# load model (only done first time)\n",
    "model_dir = snapshot_download('iic/nlp_raner_named-entity-recognition_chinese-base-generic', cache_dir='/scratch/ssd004/scratch/cambish/CNER')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-robert",
   "metadata": {},
   "source": [
    "### Create NER pipeline and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "powerful-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 14:22:31,334 - modelscope - INFO - initialize model from /scratch/ssd004/scratch/cambish/CNER/iic/nlp_raner_named-entity-recognition_chinese-base-generic\n",
      "2024-10-10 14:22:38,720 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "/h/cambish/.conda/envs/modelscope/lib/python3.9/site-packages/modelscope/utils/checkpoint.py:550: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_file, map_location='cpu')\n",
      "2024-10-10 14:22:47,096 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTokenClassificationWithCRF.\n",
      "\n",
      "2024-10-10 14:22:47,100 - modelscope - INFO - All the weights of ModelForTokenClassificationWithCRF were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTokenClassificationWithCRF for predictions without further training.\n",
      "2024-10-10 14:22:48,311 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-10-10 14:22:48,318 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-10-10 14:22:48,328 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/scratch/ssd004/scratch/cambish/CNER/iic/nlp_raner_named-entity-recognition_chinese-base-generic'}. trying to build by task and model information.\n",
      "2024-10-10 14:22:59,299 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-10-10 14:22:59,630 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-10-10 14:22:59,640 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/scratch/ssd004/scratch/cambish/CNER/iic/nlp_raner_named-entity-recognition_chinese-base-generic', 'sequence_length': 512}. trying to build by task and model information.\n",
      "2024-10-10 14:22:59,825 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-10-10 14:22:59,834 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-10-10 14:22:59,835 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/scratch/ssd004/scratch/cambish/CNER/iic/nlp_raner_named-entity-recognition_chinese-base-generic', 'sequence_length': 512}. trying to build by task and model information.\n",
      "/h/cambish/.conda/envs/modelscope/lib/python3.9/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'type': 'PER', 'start': np.int64(0), 'end': np.int64(3), 'prob': np.float32(0.6087498), 'span': '尼克松'}, {'type': 'GPE', 'start': np.int64(12), 'end': np.int64(13), 'prob': np.float32(0.9466083), 'span': '华'}, {'type': 'PER', 'start': np.int64(30), 'end': np.int64(37), 'prob': np.float32(0.6117986), 'span': '福斯特·杜勒斯'}, {'type': 'GPE', 'start': np.int64(41), 'end': np.int64(43), 'prob': np.float32(0.6031834), 'span': '美国'}, {'type': 'PER', 'start': np.int64(50), 'end': np.int64(51), 'prob': np.float32(0.93963814), 'span': '周'}, {'type': 'PER', 'start': np.int64(52), 'end': np.int64(55), 'prob': np.float32(0.43081495), 'span': '周恩来'}]}, {'output': [{'type': 'PER', 'start': np.int64(0), 'end': np.int64(3), 'prob': np.float32(0.64598316), 'span': '尼克松'}, {'type': 'PER', 'start': np.int64(4), 'end': np.int64(7), 'prob': np.float32(0.32906184), 'span': '周恩来'}, {'type': 'GPE', 'start': np.int64(12), 'end': np.int64(14), 'prob': np.float32(0.58992296), 'span': '美国'}, {'type': 'GPE', 'start': np.int64(21), 'end': np.int64(23), 'prob': np.float32(0.53913224), 'span': '中国'}]}, {'output': [{'type': 'PER', 'start': np.int64(0), 'end': np.int64(3), 'prob': np.float32(0.6675584), 'span': '尼克松'}, {'type': 'PER', 'start': np.int64(6), 'end': np.int64(13), 'prob': np.float32(0.5807448), 'span': '福斯特·杜勒斯'}, {'type': 'GPE', 'start': np.int64(15), 'end': np.int64(16), 'prob': np.float32(0.9671561), 'span': '华'}]}, {'output': [{'type': 'GPE', 'start': np.int64(0), 'end': np.int64(1), 'prob': np.float32(0.95011985), 'span': '中'}, {'type': 'GPE', 'start': np.int64(1), 'end': np.int64(2), 'prob': np.float32(0.9670487), 'span': '美'}]}, {'output': []}]\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "from modelscope.models import Model\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# set up tokenization from https://huggingface.co/docs/datasets/en/use_dataset?\n",
    "# model seems to work fine without specifying tokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained()\n",
    "cfp = load_dataset('/scratch/ssd004/scratch/cambish/cmmlu-v1.0.1/cmmlu.py', \"chinese_foreign_policy\", split=\"dev\")\n",
    "\n",
    "ner_model = Model.from_pretrained(model_dir)\n",
    "ner_pipeline = pipeline(Tasks.named_entity_recognition, model=ner_model)\n",
    "\n",
    "# test sample\n",
    "ner_input = ['尼克松在回顾1972年访华时说：“我知道，1954年在……时福斯特·杜勒斯（当时的美国国务卿）拒绝同周（周恩来）握手，使他身受侮辱。因此我走完梯级时决心伸出我的手，一边向他走去。当我们的手相握的时候，一个时代结束了，另一个时代开始了。”对上述材料理解正确的是',\n",
    "            '尼克松与周恩来握手意味着美国彻底放弃了遏制中国政策', '尼克松否定了福斯特·杜勒斯的对华态度', '中美关系对20世纪70年代的国际关系有重要影响', '省略部分应是万隆会议']\n",
    "outputs = ner_pipeline(ner_input)\n",
    "print(outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96675950",
   "metadata": {},
   "source": [
    "### Visualize Output w/ SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wound-order",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/cambish/.conda/envs/modelscope/lib/python3.9/site-packages/spacy/displacy/__init__.py:213: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    尼克松\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "在回顾1972年访\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    华\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "时说：“我知道，1954年在……时\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    福斯特·杜勒斯\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "（当时的\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    美国\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "国务卿）拒绝同\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    周\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "（\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    周恩来\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "）握手，使他身受侮辱。因此我走完梯级时决心伸出我的手，一边向他走去。当我们的手相握的时候，一个时代结束了，另一个时代开始了。”对上述材料理解正确的是</div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    尼克松\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "与\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    周恩来\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "握手意味着\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    美国\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "彻底放弃了遏制\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    中国\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "政策</div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    尼克松\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "否定了\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    福斯特·杜勒斯\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "的对\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    华\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "态度</div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    中\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    美\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "关系对20世纪70年代的国际关系有重要影响</div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">省略部分应是万隆会议</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.blank(\"zh\")\n",
    "\n",
    "# docs list will store annotations\n",
    "docs = []\n",
    "for text, annotations in zip(ner_input, outputs):\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    # required to convert annotations dict of lists into annotation dict\n",
    "    for annotation in annotations['output']:\n",
    "        span = doc.char_span(annotation['start'], annotation['end'], label=annotation['type'])\n",
    "        ents.append(span)\n",
    "    doc.ents = ents\n",
    "    docs.append(doc)\n",
    "\n",
    "displacy.render(docs, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "japanese-warrant",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for './'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m cfp \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scratch/ssd004/scratch/cambish/cmmlu-v1.0.1/cmmlu.py\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchinese_foreign_policy\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/modelscope/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2192\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2190\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2193\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2196\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2197\u001b[0m     )\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for './'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
